**YouTube Sentiment Analysis Library** — это библиотека на Python для анализа тональности текстовых комментариев с YouTube. Она позволяет классифицировать комментарии по трем категориям: `positive` (положительные), `neutral` (нейтральные) и `negative` (негативные). Проект использует методы машинного обучения, такие как логистическая регрессия и наивный Байес, для обучения моделей, а также предоставляет удобный интерфейс для предсказания тональности новых комментариев.

Библиотека поддерживает полный цикл обработки текстов: от предобработки (очистка, токенизация, векторизация) до обучения модели, её оценки и использования для предсказаний. Проект модульный, что позволяет легко добавлять новые модели или методы обработки текста.

## Основные возможности

- **Предобработка текста**:
  - Очистка текста: приведение к нижнему регистру, удаление пунктуации и цифр, удаление стоп-слов с использованием NLTK.
  - Базовая токенизация текста.
  - Векторизация текста с использованием двух методов:
    - Bag of Words (BoW).
    - TF-IDF.
- **Обучение модели**:
  - Поддержка двух моделей классификации: логистическая регрессия и наивный Байес.
  - Обучение на трех классах: `positive`, `neutral`, `negative`.
  - Кросс-валидация для оценки стабильности модели.
- **Оценка модели**:
  - Вычисление метрик: `accuracy`, `precision`, `recall`, `F1-score` для каждого класса.
  - Визуализация результатов:
    - Матрица ошибок (Confusion Matrix).
    - График важности слов (Feature Importance).
- **Дополнительные функции**:
  - Сохранение и загрузка обученной модели и векторизатора.
  - Удобный интерфейс для предсказания тональности новых комментариев.
  - Модульная структура, позволяющая легко подключать новые модели или методы обработки.

## Предварительные требования

Для работы с проектом вам понадобится:
- **Python 3.10+** (рекомендуется использовать последнюю версию для совместимости с библиотеками).
- Установленные зависимости:
  - `pandas` — для работы с данными.
  - `scikit-learn` — для машинного обучения и векторизации текста.
  - `nltk` — для предобработки текста (удаление стоп-слов, токенизация).
  - `matplotlib` — для визуализации результатов.
  - `joblib` — для сохранения и загрузки моделей.

## Структура проекта

├── youtube_comments.csv         # Датасет с комментариями
├── train.py                     # Скрипт для обучения модели
├── predict.py                   # Скрипт для предсказания тональности
├── requirements.txt             # Файл с зависимостями
├── utils/
│   └── io.py                    # Утилиты для ввода/вывода данных
├── preprocessing/
│   ├── text_cleaner.py          # Модуль для очистки текста
│   └── tokenizer_vectorizer.py  # Модуль для токенизации и векторизации
├── models/
│   └── logistic_model.py        # Модель для анализа тональности
├── metrics/
│   └── evaluation.py            # Модуль для оценки модели
└── README.md                    # Описание проекта


## Установка

Следуйте этим шагам, чтобы настроить проект на вашем компьютере.

### 1. Клонирование репозитория
Склонируйте репозиторий с GitHub:
```bash
git clone https://github.com/DerChair/YouTube-Sentiment-Analysis-Library.git
cd YouTube-Sentiment-Analysis-Library
```





### 2. Создание виртуального окружения (рекомендуется)
Чтобы избежать конфликтов с другими проектами, создайте виртуальное окружение:

```bash
python -m venv venv
```
Активируйте его:

```bash
venv\Scripts\activate
```

```bash
source venv/bin/activate
```
После активации вы увидите (venv) в начале строки терминала.

### 3. Установка зависимостей
Установите все необходимые библиотеки из файла requirements.txt:
```bash
pip install -r requirements.txt
```
Файл requirements.txt содержит:

pandas
scikit-learn
nltk
matplotlib
joblib
### 4. Загрузка данных NLTK
Проект использует NLTK для обработки текста. Загрузите необходимые данные:

```bash
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
stopwords: для удаления стоп-слов.
punkt: для токенизации текста.
```
### 5. Подготовка данных
Проект ожидает файл с данными youtube_comments.csv в корневой директории. Формат файла:

```bash
Comment,Sentiment
"lets not forget that apple pay in 2014 required a brand new iphone ...,neutral"
"here in nz 50 of retailers don’t even have contactless credit card machines ...,negative"
```
Столбец Comment,Sentiment содержит текст комментария и его тональность, разделенные запятой.
Если у вас нет этого файла, создайте его или используйте другой датасет в аналогичном формате.

# Загрузка модели
```bash
analyzer = SentimentAnalyzer()
analyzer.load('sentiment_model.pkl', 'vectorizer.pkl')
```
# Предсказание
```bash
comment = "This phone is amazing!"
cleaner = TextCleaner()
cleaned_comment = cleaner.clean_text(comment)
prediction = analyzer.predict([cleaned_comment])[0]
probabilities = analyzer.predict_proba([cleaned_comment])[0]
print(f"Sentiment: {prediction}")
print(f"Probabilities (negative, neutral, positive): {probabilities}")
```
